{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNjvwGEr-Z6R"
   },
   "source": [
    "## mbqr_rag_scratchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRO7Z6Mf-gcO"
   },
   "source": [
    "_WIP - NOT FOR DISTRIBUTION_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mmGZEHn-m8G"
   },
   "source": [
    "> `mbqr_rag_scratchpad.ipynb`<br>\n",
    "> Simone J. Skeen (08-10-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtafTG3v9_ln"
   },
   "source": [
    "### 1. Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfW2EPkQ-uvi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install langchain faiss-cpu tiktoken chromadb\n",
    "#!pip install --upgrade langchain-community\n",
    "!pip install --upgrade langchain langchain-core langchain-community pydantic==2.6.4\n",
    "!pip install pymupdf\n",
    "!pip install pypdf\n",
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda Prompt (anaconda3) > `ollama pull nomic-embed-text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eIUcelv-wZx"
   },
   "outputs": [],
   "source": [
    "import numpy as np, os, pandas as pd, streamlit as st, warnings\n",
    "\n",
    "import langchain, pydantic\n",
    "print(\"LangChain:\", langchain.__version__)\n",
    "print(\"Pydantic:\", pydantic.__version__)\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_columns',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_rows',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "for c in (FutureWarning, UserWarning):\n",
    "    warnings.simplefilter(\n",
    "        action = 'ignore',\n",
    "        category = c,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write\n",
    "Defines requisite custom functions: `rag_tune` module.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd (local)\n",
    "\n",
    "CODE_DIR = 'C:/Users/sskee/OneDrive/Documents/01_brown/active/mbqr_rag/code'\n",
    "os.chdir(CODE_DIR)\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `build_retrieval_qa_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rag_tune.py\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def build_retrieval_qa_chain(\n",
    "    llm,\n",
    "    retriever,\n",
    "    chain_type = 'stuff',\n",
    "    *,\n",
    "    prompt = None,\n",
    "    question_prompt = None,\n",
    "    refine_prompt = None,\n",
    "    combine_prompt = None,\n",
    "    ):\n",
    "    \n",
    "    '''\n",
    "    Build a RetrievalQA chain with support for external prompt injection.\n",
    "\n",
    "    Parameters:\n",
    "    - llm: the LLM object (e.g., Ollama instance)\n",
    "    - retriever: a LangChain retriever (e.g., FAISS)\n",
    "    - chain_type: 'stuff', 'map_reduce', or 'refine'\n",
    "    - prompt: for 'stuff' chains\n",
    "    - question_prompt & refine_prompt: for 'refine' chains\n",
    "    - question_prompt & combine_prompt: for 'map_reduce' chains\n",
    "    '''\n",
    "\n",
    "    if chain_type == 'stuff':\n",
    "        prompt = prompt or PromptTemplate(\n",
    "            input_variables=['context', 'question'],\n",
    "            template = '''\n",
    "    You are a knowledgable conversational agent that offers accurate, succinct, responses \n",
    "        based on the provided context.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    '''\n",
    "        )\n",
    "        \n",
    "        chain_type_kwargs = {'prompt': prompt}\n",
    "\n",
    "    elif chain_type == 'map_reduce':\n",
    "        question_prompt = question_prompt or PromptTemplate(\n",
    "            input_variables = ['context', 'question'],\n",
    "            template = '''\n",
    "    Examine the following context to respond to the query as accurately as possible.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    '''\n",
    "        )\n",
    "        \n",
    "        combine_prompt = combine_prompt or PromptTemplate(\n",
    "            input_variables = ['summaries', 'question'],\n",
    "            template = '''\n",
    "    You are a knowledgable conversational agent that synthesizes multiple responses to \n",
    "        create a single comprehensive response.\n",
    "\n",
    "    Summaries:\n",
    "    {summaries}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Final Answer:\n",
    "    '''\n",
    "        )\n",
    "        \n",
    "        chain_type_kwargs = {\n",
    "            'question_prompt': question_prompt,\n",
    "            'combine_prompt': combine_prompt,\n",
    "            }\n",
    "\n",
    "    elif chain_type == 'refine':\n",
    "        question_prompt = question_prompt or PromptTemplate(\n",
    "            input_variables = ['context', 'question'],\n",
    "            template = '''\n",
    "    You are a knowledgable conversational agent that offers accurate, succinct, responses \n",
    "        based on the provided context.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    '''\n",
    "        )\n",
    "        \n",
    "        refine_prompt = refine_prompt or PromptTemplate(\n",
    "            input_variables = ['context', 'question', 'existing_answer'],\n",
    "            template = '''\n",
    "        You are improving an existing response using new context.\n",
    "\n",
    "        Existing Answer:\n",
    "        {existing_answer}\n",
    "\n",
    "        New Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Refined Answer:\n",
    "        '''\n",
    "        )\n",
    "        chain_type_kwargs = {\n",
    "            'question_prompt': question_prompt,\n",
    "            'refine_prompt': refine_prompt\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported chain_type: {chain_type}\")\n",
    "\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        retriever = retriever,\n",
    "        chain_type = chain_type,\n",
    "        return_source_documents = True,\n",
    "        chain_type_kwargs = chain_type_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `query_and_stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a rag_tune.py\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def query_and_stream(\n",
    "    llm,\n",
    "    retriever,\n",
    "    query,\n",
    "    prompt_template = None,\n",
    "    show_sources = True,\n",
    "    ):\n",
    "    \n",
    "    '''\n",
    "    Wrapper that replicates RetrievalQA (chain_type = 'stuff') behavior with token streaming.\n",
    "\n",
    "    Parameters:\n",
    "    - llm: Ollama LLM (streamable, e.g. Deepseek-R1)\n",
    "    - retriever: pre-specfied (external) vector store retriever\n",
    "    - query: user question\n",
    "    - prompt_template: optional PromptTemplate (context + question)\n",
    "    - show_sources: determines whether to print the source documents after the answer\n",
    "    '''\n",
    "\n",
    "    # define PromptTemplate if none is passed\n",
    "    \n",
    "    if prompt_template is None:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables = ['context', 'question'],\n",
    "            template = '''\n",
    "    You are a knowledgable conversational agent that offers accurate, succinct, responses \n",
    "        based on the provided context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:'''\n",
    "            \n",
    "        )\n",
    "\n",
    "    # manually replicate RetrievalQA behavior (chain_type = 'stuff')\n",
    "\n",
    "    #docs = retriever.get_relevant_documents(query) ### deprecated\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        print(\"No relevant documents found.\")\n",
    "        return\n",
    "\n",
    "    # concatenate ('stuff') context\n",
    "    \n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # format prompt\n",
    "    \n",
    "    prompt = prompt_template.format(\n",
    "        context = context, \n",
    "        question = query,\n",
    "    )\n",
    "\n",
    "    # stream response token-by-token\n",
    "    \n",
    "    print(\"\\n🔮\\n\")\n",
    "    for token in llm.stream(prompt):\n",
    "        print(\n",
    "            token, \n",
    "            end = \"\", \n",
    "            flush = True,\n",
    "            )\n",
    "\n",
    "    print(\"\\n\\n🍂\")\n",
    "\n",
    "    # print source metadata (optional)\n",
    "    \n",
    "    if show_sources:\n",
    "        print(\"\\nknowledge excerpts:\\n\")\n",
    "        for i, doc in enumerate(docs):\n",
    "            \n",
    "        ### SJS 8/9: verbose w/ page_content...\n",
    "            \n",
    "            meta = doc.metadata\n",
    "            print(f\"--- excerpt {i+1} ---\")\n",
    "            print(f\"metadata: {meta}\")\n",
    "            print(doc.page_content[:1000], \"...\\n\")\n",
    "            \n",
    "        ### SJS 8/9: cleaner - metadata _only_\n",
    "        \n",
    "            #yield f\"\\n[{i+1}] {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}\"       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pwd\n",
    "from rag_tune import(\n",
    "    build_retrieval_qa_chain,\n",
    "    query_and_stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess + Initialize\n",
    "Loads, splits, chunks knowledge base documents; initializes LLM, retriever; vectorizes.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Load, inspect_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU7giumc-wdQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# (re)set wd\n",
    "\n",
    "KNOW_DIR = 'C:/Users/sskee/OneDrive/Documents/01_brown/active/mbqr_rag/knowledge'\n",
    "\n",
    "# load pdf knowledge base\n",
    "\n",
    "pdf_paths = [\n",
    "    os.path.join(KNOW_DIR, \"mbqr_manual_rag_db.pdf\"),\n",
    "    os.path.join(KNOW_DIR, \"mbqr_scripts_rag_db.pdf\"),\n",
    "    os.path.join(KNOW_DIR, \"poems_of protest_resistance_empowerment_rag_db_prelim.pdf\"),\n",
    "    ]\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "# group pdf by file\n",
    "\n",
    "for path in pdf_paths:\n",
    "#    loader = PyPDFLoader(path)\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    all_documents.append(docs) ### append \"list of lists\"\n",
    "\n",
    "    print(f\"{os.path.basename(path)}: {len(docs)} p/p. loaded\")\n",
    "    \n",
    "# spot check: leading 500 characters, each file\n",
    "\n",
    "for i, doc_pages in enumerate(all_documents):\n",
    "    print(f\"\\n--------------- knowledge source {i+1} ---------------\")\n",
    "    print(f\"\\n\")\n",
    "    print(doc_pages[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Vectorize_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten into single list\n",
    "\n",
    "flat_documents = [page for doc in all_documents for page in doc]\n",
    "\n",
    "# chunk into \"chunked\" excerpts\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 200, \n",
    "    separators = ['\\n\\n', '\\n', ' ', ''],\n",
    "    )\n",
    "\n",
    "chunked_documents = splitter.split_documents(flat_documents)\n",
    "print(f\"chunks chunked: {len(chunked_documents)}\")\n",
    "\n",
    "# embed + create FAISS vector db\n",
    "\n",
    "embedding = OllamaEmbeddings(model = \"nomic-embed-text\")\n",
    "db = FAISS.from_documents(chunked_documents, embedding)\n",
    "\n",
    "# save\n",
    "\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load locally saved vector db\n",
    "\n",
    "#db = FAISS.load_local(\n",
    "#    \"faiss_index\", \n",
    "#    embeddings = embedding,\n",
    "#    allow_dangerous_deserialization = True,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Input_ `system_prompt` 💭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "    You are an agent with access to very high quality evidence-based mindfulness skills instruction in your provided context. \n",
    "    You will be prompted with everyday stressors and problems. Your task is to:\n",
    "    \n",
    "        1.) search your provided context,\n",
    "        2.) summarize in-context knowledge on stress and resilience,\n",
    "        3.) recommend specific skills and practices that might benefit the user _given_ their reported stressors.\n",
    "        \n",
    "    - ALWAYS consult your context first when responding. \n",
    "    - NEVER return recommendations from sources other than your context. \n",
    "    - You are warm, empowering, and prioritize empathy in your tone and response contents. \n",
    "    - You maintain a sixth-grade reading level in your responses. \n",
    "    - Do not assume the user is LGBTQ+\n",
    "    - You are concise: you limit responses to 200 words.\n",
    "    - If prompted for an inspiring quote, curate from the poetry in your context.\n",
    "    - Refer to your context as your \"mindfulness knowledge.\" Do NOT refer to your \"context.\"\n",
    "    - At the close of each response, encourage the user to practice the recommended skill.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Input_ `query` 💬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    I sometimes struggle with negative feelings toward my body\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     I have too much to deal with today! I feel so overwhelmed I can't even start\n",
    "#     I sometimes struggle with negative feelings toward my body\n",
    "#     Please tell me an inspiring quote. The world feels like too much lately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Configure LLM, PromptTemplate_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda Prompt (anaconda3) > `ollama pull deepseek-r1:14b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default (`chain_type = 'stuff'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXBJo7Xv-wkS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config llm via ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model = 'deepseek-r1:14b', ### model tag for app: 'deepseek-v2' (16b); for dx / reasoning: 'deepseek-r1:14b' (14b)\n",
    "    base_url = 'http://localhost:11434',\n",
    "    temperature = 0.6, ### args / params: https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html\n",
    "    mirostat_eta = 0.1,\n",
    "    mirostat_tau = 5.0,\n",
    "    top_p = 0.9,\n",
    "    top_k = 40,\n",
    "    num_ctx = 2048,\n",
    "    num_gpu = 1,\n",
    "    num_predict = 768,\n",
    "    repeat_last_n = 64,\n",
    "    stop = None,\n",
    "    )\n",
    "\n",
    "# config prompttemplate (default)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context', 'question'],\n",
    "    template = '''\n",
    "        {system_prompt}\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "        '''.strip(),\n",
    "    ).partial(system_prompt = system_prompt)\n",
    "\n",
    "# set up retriever\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type = 'similarity', ### cosine similarity / vector distance\n",
    "    search_kwargs = {'k': 4}, ### top k = 4 chunks based on doc similarity w/in FAISS vector store\n",
    "    )\n",
    "\n",
    "        ### SJS 8/7: alternate search_type options below...custom fx tktk...\n",
    "\n",
    "#retriever = db.as_retriever(\n",
    "#    search_type = 'mmr',\n",
    "#    search_kwargs = {\n",
    "#        'k': 10, \n",
    "#        'fetch_k': 20, \n",
    "#        'lambda_mult': 0.5,\n",
    "#        }\n",
    "#    )\n",
    "\n",
    "#retriever = db.as_retriever(\n",
    "#    search_type = 'similarity_score_threshold',\n",
    "#    search_kwargs = {'score_threshold': 0.8},\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bgGdrW8-wne"
   },
   "outputs": [],
   "source": [
    "# non-streaming qa_chain config\n",
    "\n",
    "#qa_chain = RetrievalQA.from_chain_type(\n",
    "#    llm = llm,\n",
    "#    retriever = retriever,\n",
    "#    chain_type = 'stuff', \n",
    "#    return_source_documents = True,\n",
    "#    chain_type_kwargs = {'prompt': prompt},\n",
    "#    )\n",
    "\n",
    "# query\n",
    "\n",
    "#query = query\n",
    "#result = qa_chain({\"query\": query})\n",
    "\n",
    "#print(\"Answer:\", result['result'])\n",
    "\n",
    "#for doc in result['source_documents']:\n",
    "#    print(\"\\nSource:\", doc.metadata)\n",
    "#    print(doc.page_content[:500])  # Truncated content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Query\n",
    "Prompt QA chain, inspect / rate response.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_and_stream(\n",
    "    llm, \n",
    "    retriever, \n",
    "    query,\n",
    "    prompt_template = prompt,\n",
    "    show_sources = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> End of mbqr_rag_scratchpad.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

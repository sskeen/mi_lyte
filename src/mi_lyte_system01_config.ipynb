{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNjvwGEr-Z6R"
   },
   "source": [
    "## ðŸ”® mÄ« lyte: RAG architectures + config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRO7Z6Mf-gcO"
   },
   "source": [
    "_WIP - NOT FOR DISTRIBUTION_\n",
    "\n",
    "_Backend notebook: handles retrieval-augemented generation (RAG) initialization, (local) model selection via Ollama, fine-tuning, configuration, and knowledge base curation, for_ ðŸ‚ _mÄ« lyte System 1: an evidence-based informal mindfulness skills + intrapersonal resilience asset recommendation engine._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mmGZEHn-m8G"
   },
   "source": [
    "> `mi_lyte_system01_config.ipynb`<br>\n",
    "> Simone J. Skeen x Claude Code (02-02-2026)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtafTG3v9_ln"
   },
   "source": [
    "### 1. Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***\n",
    "> **Dependencies:** i.) Install via `%pip install -r requirements.txt` from project root before running; ii.) Pull embedding model (terminal) via `ollama pull nomic-embed-text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eIUcelv-wZx"
   },
   "outputs": [],
   "source": [
    "import numpy as np, os, pandas as pd, streamlit as st, warnings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "import langchain, pydantic\n",
    "print(\"LangChain:\", langchain.__version__)\n",
    "print(\"Pydantic:\", pydantic.__version__)\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "#from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_columns',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_rows',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "for c in (FutureWarning, UserWarning):\n",
    "    warnings.simplefilter(\n",
    "        action = 'ignore',\n",
    "        category = c,\n",
    "        )\n",
    "\n",
    "# load environment variables from project root .env\n",
    "\n",
    "load_dotenv(Path('..') / '.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import\n",
    "Imports standalone `dialogue_stream.py` > `query_and_stream()` function.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd to src/ (notebook's own directory)\n",
    "\n",
    "CODE_DIR = Path().resolve()\n",
    "os.chdir(CODE_DIR)\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dialogue_stream import query_and_stream # type: ignore\nfrom config import SYSTEM_PROMPT, LLM_PARAMS, EMBEDDING_MODEL, RETRIEVER_PARAMS, PROMPT_TEMPLATE # type: ignore"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess + Initialize\n",
    "Loads, splits, chunks knowledge base documents; initializes LLM, retriever; vectorizes.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Load, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU7giumc-wdQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# knowledge base directory (loaded from .env)\n",
    "\n",
    "KNOW_DIR = os.path.expanduser(os.environ['KNOW_DIR'])\n",
    "\n",
    "# load pdf knowledge base\n",
    "\n",
    "pdf_paths = [\n",
    "    os.path.join(KNOW_DIR, \"mbqr_manual_rag_db.pdf\"),\n",
    "    os.path.join(KNOW_DIR, \"mbqr_scripts_rag_db.pdf\"),\n",
    "    os.path.join(KNOW_DIR, \"poems_of protest_resistance_empowerment_rag_db_prelim.pdf\"),\n",
    "    ]\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "# group pdf by file\n",
    "\n",
    "for path in pdf_paths:\n",
    "#    loader = PyPDFLoader(path)\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    all_documents.append(docs) ### append \"list of lists\"\n",
    "\n",
    "    print(f\"{os.path.basename(path)}: {len(docs)} p/p. loaded\")\n",
    "    \n",
    "# spot check: leading 500 characters, each file\n",
    "\n",
    "for i, doc_pages in enumerate(all_documents):\n",
    "    print(f\"\\n--------------- knowledge source {i+1} ---------------\")\n",
    "    print(f\"\\n\")\n",
    "    print(doc_pages[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# flatten into single list\n\nflat_documents = [page for doc in all_documents for page in doc]\n\n# chunk into \"chunked\" excerpts\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size = 1000, \n    chunk_overlap = 200, \n    separators = ['\\n\\n', '\\n', ' ', ''],\n    )\n\nchunked_documents = splitter.split_documents(flat_documents)\nprint(f\"chunks chunked: {len(chunked_documents)}\")\n\n# embed + create FAISS vector db\n\nembedding = OllamaEmbeddings(model = EMBEDDING_MODEL)\ndb = FAISS.from_documents(chunked_documents, embedding)\n\n# save\n\ndb.save_local(\"faiss_index\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load locally saved vector db\n",
    "\n",
    "#db = FAISS.load_local(\n",
    "#    \"faiss_index\", \n",
    "#    embeddings = embedding,\n",
    "#    allow_dangerous_deserialization = True,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 3c. `system_prompt` + `PromptTemplate()` ðŸ’­\nImported from `config.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(SYSTEM_PROMPT)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Input ðŸ’¬ `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    I sometimes struggle with negative feelings toward my body\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     I have too much to deal with today! I feel so overwhelmed I can't even start\n",
    "#     I sometimes struggle with negative feelings toward my body\n",
    "#     Please tell me an inspiring quote. The world feels like too much lately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3e. Configure LLM, `PromptTemplate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda Prompt (anaconda3) > `ollama pull deepseek-r1:14b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default (`chain_type = 'stuff'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXBJo7Xv-wkS",
    "tags": []
   },
   "outputs": [],
   "source": "# config llm via ollama\n\nllm = Ollama(**LLM_PARAMS)\n\n# config prompttemplate (default)\n\nprompt = PROMPT_TEMPLATE\n\n# set up retriever\n\nretriever = db.as_retriever(**RETRIEVER_PARAMS)\n\n        ### SJS 8/7: alternate search_type options below...custom fx tktk...\n\n#retriever = db.as_retriever(\n#    search_type = 'mmr',\n#    search_kwargs = {\n#        'k': 10, \n#        'fetch_k': 20, \n#        'lambda_mult': 0.5,\n#        }\n#    )\n\n#retriever = db.as_retriever(\n#    search_type = 'similarity_score_threshold',\n#    search_kwargs = {'score_threshold': 0.8},\n#    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bgGdrW8-wne"
   },
   "outputs": [],
   "source": [
    "# non-streaming qa_chain config\n",
    "\n",
    "#qa_chain = RetrievalQA.from_chain_type(\n",
    "#    llm = llm,\n",
    "#    retriever = retriever,\n",
    "#    chain_type = 'stuff', \n",
    "#    return_source_documents = True,\n",
    "#    chain_type_kwargs = {'prompt': prompt},\n",
    "#    )\n",
    "\n",
    "# query\n",
    "\n",
    "#query = query\n",
    "#result = qa_chain({\"query\": query})\n",
    "\n",
    "#print(\"Answer:\", result['result'])\n",
    "\n",
    "#for doc in result['source_documents']:\n",
    "#    print(\"\\nSource:\", doc.metadata)\n",
    "#    print(doc.page_content[:500])  # Truncated content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Query\n",
    "Prompt QA chain, inspect / rate response.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_and_stream(\n",
    "    llm, \n",
    "    retriever, \n",
    "    query,\n",
    "    prompt_template = prompt,\n",
    "    show_sources = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> End of mi_lyte_system01_config.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
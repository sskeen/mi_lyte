{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNjvwGEr-Z6R"
   },
   "source": [
    "## ðŸ”® mÄ« lyte: RAG architectures + config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRO7Z6Mf-gcO"
   },
   "source": [
    "_WIP - NOT FOR DISTRIBUTION_\n",
    "\n",
    "_Backend notebook: handles retrieval-augemented generation (RAG) initialization, (local) model selection via Ollama, fine-tuning, configuration, and knowledge base curation, for_ ðŸ‚ _mÄ« lyte System 1: an evidence-based informal mindfulness skills + intrapersonal resilience asset recommendation engine._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mmGZEHn-m8G"
   },
   "source": [
    "> `mi_lyte_system01_config.ipynb`<br>\n",
    "> Simone J. Skeen x Claude Code (02-02-2026)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtafTG3v9_ln"
   },
   "source": [
    "### 1. Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***\n",
    "> **Dependencies:** i.) Install via `%pip install -r requirements.txt` from project root before running; ii.) Pull embedding model (terminal) via `ollama pull nomic-embed-text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%%capture\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eIUcelv-wZx"
   },
   "outputs": [],
   "source": [
    "import numpy as np, os, pandas as pd, streamlit as st, warnings\n",
    "\n",
    "import langchain, pydantic\n",
    "print(\"LangChain:\", langchain.__version__)\n",
    "print(\"Pydantic:\", pydantic.__version__)\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "#from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_columns',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_rows',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "for c in (FutureWarning, UserWarning):\n",
    "    warnings.simplefilter(\n",
    "        action = 'ignore',\n",
    "        category = c,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write\n",
    "Defines standalone `query_and_stream.py` function.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd (local)\n",
    "\n",
    "CODE_DIR = '/Users/sskeen/anaconda_projects/mi_lyte/src'\n",
    "os.chdir(CODE_DIR)\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dialogue_stream.py\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def query_and_stream(\n",
    "    llm,\n",
    "    retriever,\n",
    "    query,\n",
    "    prompt_template = None,\n",
    "    show_sources = True,\n",
    "    ):\n",
    "    \n",
    "    '''\n",
    "    Wrapper that replicates RetrievalQA (chain_type = 'stuff') behavior with token streaming.\n",
    "\n",
    "    Parameters:\n",
    "    - llm: Ollama LLM (streamable, e.g. Deepseek-R1)\n",
    "    - retriever: pre-specfied (external) vector store retriever\n",
    "    - query: user question\n",
    "    - prompt_template: optional PromptTemplate (context + question)\n",
    "    - show_sources: determines whether to print the source documents after the answer\n",
    "    '''\n",
    "\n",
    "    # define PromptTemplate if none is passed\n",
    "    \n",
    "    if prompt_template is None:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables = ['context', 'question'],\n",
    "            template = '''\n",
    "    You are a knowledgable conversational agent that offers accurate, succinct, responses \n",
    "        based on the provided context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:'''\n",
    "            \n",
    "        )\n",
    "\n",
    "    # manually replicate RetrievalQA behavior (chain_type = 'stuff')\n",
    "\n",
    "    #docs = retriever.get_relevant_documents(query) ### deprecated\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        print(\"No relevant documents found.\")\n",
    "        return\n",
    "\n",
    "    # concatenate ('stuff') context\n",
    "    \n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # format prompt\n",
    "    \n",
    "    prompt = prompt_template.format(\n",
    "        context = context, \n",
    "        question = query,\n",
    "    )\n",
    "\n",
    "    # stream response token-by-token\n",
    "    \n",
    "    print(\"\\nðŸ‚\\n\")\n",
    "    for token in llm.stream(prompt):\n",
    "        print(\n",
    "            token, \n",
    "            end = \"\", \n",
    "            flush = True,\n",
    "            )\n",
    "\n",
    "    print(\"\\n\\nðŸ‚\")\n",
    "\n",
    "    # print source metadata (optional)\n",
    "    \n",
    "    if show_sources:\n",
    "        print(\"\\nknowledge excerpts:\\n\")\n",
    "        for i, doc in enumerate(docs):\n",
    "            \n",
    "        ### SJS 8/9: verbose w/ page_content...\n",
    "            \n",
    "            meta = doc.metadata\n",
    "            print(f\"--- excerpt {i+1} ---\")\n",
    "            print(f\"metadata: {meta}\")\n",
    "            print(doc.page_content[:1000], \"...\\n\")\n",
    "            \n",
    "        ### SJS 8/9: cleaner - metadata _only_\n",
    "        \n",
    "            #yield f\"\\n[{i+1}] {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}\"       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pwd\n",
    "from dialogue_stream import query_and_stream # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess + Initialize\n",
    "Loads, splits, chunks knowledge base documents; initializes LLM, retriever; vectorizes.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Load, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU7giumc-wdQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (re)set wd\n",
    "\n",
    "KNOW_DIR = '~/Documents/01_brown/active/mi_lyte/knowledge'\n",
    "\n",
    "# load pdf knowledge base\n",
    "\n",
    "pdf_paths = [\n",
    "    os.path.join(KNOW_DIR, \"mbqr_manual_rag_db.pdf\"),\n",
    "    os.path.join(KNOW_DIR, \"mbqr_scripts_rag_db.pdf\"),\n",
    "    os.path.join(KNOW_DIR, \"poems_of protest_resistance_empowerment_rag_db_prelim.pdf\"),\n",
    "    ]\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "# group pdf by file\n",
    "\n",
    "for path in pdf_paths:\n",
    "#    loader = PyPDFLoader(path)\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    all_documents.append(docs) ### append \"list of lists\"\n",
    "\n",
    "    print(f\"{os.path.basename(path)}: {len(docs)} p/p. loaded\")\n",
    "    \n",
    "# spot check: leading 500 characters, each file\n",
    "\n",
    "for i, doc_pages in enumerate(all_documents):\n",
    "    print(f\"\\n--------------- knowledge source {i+1} ---------------\")\n",
    "    print(f\"\\n\")\n",
    "    print(doc_pages[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten into single list\n",
    "\n",
    "flat_documents = [page for doc in all_documents for page in doc]\n",
    "\n",
    "# chunk into \"chunked\" excerpts\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 200, \n",
    "    separators = ['\\n\\n', '\\n', ' ', ''],\n",
    "    )\n",
    "\n",
    "chunked_documents = splitter.split_documents(flat_documents)\n",
    "print(f\"chunks chunked: {len(chunked_documents)}\")\n",
    "\n",
    "# embed + create FAISS vector db\n",
    "\n",
    "embedding = OllamaEmbeddings(model = \"nomic-embed-text\")\n",
    "db = FAISS.from_documents(chunked_documents, embedding)\n",
    "\n",
    "# save\n",
    "\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load locally saved vector db\n",
    "\n",
    "#db = FAISS.load_local(\n",
    "#    \"faiss_index\", \n",
    "#    embeddings = embedding,\n",
    "#    allow_dangerous_deserialization = True,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Formulate ðŸ’­ `system_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "    Your name is \"mÄ« lyte.\" You have access to very high quality evidence-based mindfulness skills instruction in your provided context. \n",
    "    You will be prompted with everyday stressors and problems. Your task is to:\n",
    "    \n",
    "        1.) search your provided context,\n",
    "        2.) summarize in-context knowledge on stress and resilience,\n",
    "        3.) recommend specific skills and practices that might benefit the user _given_ their reported stressors.\n",
    "        \n",
    "    - ALWAYS consult your context first when responding. \n",
    "    - NEVER return recommendations from sources other than your context. \n",
    "    - You are warm, empowering, and prioritize empathy in your tone and response contents. \n",
    "    - You maintain a sixth-grade reading level in your responses. \n",
    "    - Do not assume the user is LGBTQ+\n",
    "    - You are concise: you limit responses to 100 words.\n",
    "    - If prompted for an inspiring quote, curate from the poetry in your context.\n",
    "    - Refer to your context as your \"mindfulness knowledge.\" Do NOT refer to your \"context.\"\n",
    "    - At the close of each response, encourage the user to practice the recommended skill.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Input ðŸ’¬ `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    I sometimes struggle with negative feelings toward my body\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     I have too much to deal with today! I feel so overwhelmed I can't even start\n",
    "#     I sometimes struggle with negative feelings toward my body\n",
    "#     Please tell me an inspiring quote. The world feels like too much lately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3e. Configure LLM, `PromptTemplate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda Prompt (anaconda3) > `ollama pull deepseek-r1:14b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default (`chain_type = 'stuff'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXBJo7Xv-wkS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config llm via ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model = 'deepseek-r1:14b', ### model tag for app: 'deepseek-v2' (16b); for dx / reasoning: 'deepseek-r1:14b' (14b)\n",
    "    base_url = 'http://localhost:11434',\n",
    "    temperature = 0.6, ### args / params: https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html\n",
    "    mirostat_eta = 0.1,\n",
    "    mirostat_tau = 5.0,\n",
    "    top_p = 0.9,\n",
    "    top_k = 40,\n",
    "    num_ctx = 2048,\n",
    "    num_gpu = 1,\n",
    "    num_predict = 768,\n",
    "    repeat_last_n = 64,\n",
    "    stop = None,\n",
    "    )\n",
    "\n",
    "# config prompttemplate (default)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context', 'question'],\n",
    "    template = '''\n",
    "        {system_prompt}\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "        '''.strip(),\n",
    "    ).partial(system_prompt = system_prompt)\n",
    "\n",
    "# set up retriever\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type = 'similarity', ### cosine similarity / vector distance\n",
    "    search_kwargs = {'k': 4}, ### top k = 4 chunks based on doc similarity w/in FAISS vector store\n",
    "    )\n",
    "\n",
    "        ### SJS 8/7: alternate search_type options below...custom fx tktk...\n",
    "\n",
    "#retriever = db.as_retriever(\n",
    "#    search_type = 'mmr',\n",
    "#    search_kwargs = {\n",
    "#        'k': 10, \n",
    "#        'fetch_k': 20, \n",
    "#        'lambda_mult': 0.5,\n",
    "#        }\n",
    "#    )\n",
    "\n",
    "#retriever = db.as_retriever(\n",
    "#    search_type = 'similarity_score_threshold',\n",
    "#    search_kwargs = {'score_threshold': 0.8},\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bgGdrW8-wne"
   },
   "outputs": [],
   "source": [
    "# non-streaming qa_chain config\n",
    "\n",
    "#qa_chain = RetrievalQA.from_chain_type(\n",
    "#    llm = llm,\n",
    "#    retriever = retriever,\n",
    "#    chain_type = 'stuff', \n",
    "#    return_source_documents = True,\n",
    "#    chain_type_kwargs = {'prompt': prompt},\n",
    "#    )\n",
    "\n",
    "# query\n",
    "\n",
    "#query = query\n",
    "#result = qa_chain({\"query\": query})\n",
    "\n",
    "#print(\"Answer:\", result['result'])\n",
    "\n",
    "#for doc in result['source_documents']:\n",
    "#    print(\"\\nSource:\", doc.metadata)\n",
    "#    print(doc.page_content[:500])  # Truncated content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Query\n",
    "Prompt QA chain, inspect / rate response.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_and_stream(\n",
    "    llm, \n",
    "    retriever, \n",
    "    query,\n",
    "    prompt_template = prompt,\n",
    "    show_sources = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> End of mi_lyte_system01_config.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
